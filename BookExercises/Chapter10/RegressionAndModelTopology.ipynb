{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 10 - Regression on Housing Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras \n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "housing = fetch_california_housing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_full, X_test, y_train_full, y_test = train_test_split(\n",
    "    housing.data, housing.target\n",
    ")\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X_train_full, y_train_full\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an instance of ```StandardScaler``` and call ```fit_transform``` on this instance with the data to fit the transformer to the data, and then return the transformed data instances. \n",
    "\n",
    "Same thing as calling ```.fit().transform()```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_valid = scaler.fit_transform(X_valid)\n",
    "X_test = scaler.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building a model is very similar to the classification example, except the output layer has only one neuron with no activation function. The loss function in this case will be MSE. Since the dataset is noisy, we use just one hidden layer with fewer neurons to avoid overfitting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential([\n",
    "    keras.layers.Dense(30, activation='relu', input_shape=X_train.shape[1:]),\n",
    "    keras.layers.Dense(1)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3582 - val_loss: 2.6163\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3568 - val_loss: 2.5816\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3534 - val_loss: 2.6516\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3609 - val_loss: 2.8608\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3764 - val_loss: 2.8371\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3522 - val_loss: 2.8871\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3508 - val_loss: 3.0779\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3501 - val_loss: 3.0655\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3495 - val_loss: 3.2191\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 3.2226\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3485 - val_loss: 3.4249\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3507 - val_loss: 3.5520\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3882 - val_loss: 3.3341\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3480 - val_loss: 3.5408\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3447 - val_loss: 3.3540\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3432 - val_loss: 3.7041\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3438 - val_loss: 3.8085\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3416 - val_loss: 3.9751\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3446 - val_loss: 4.1377\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3428 - val_loss: 4.0287\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='mean_squared_error', optimizer='sgd'\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 965us/step - loss: 0.3969\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_new = X_test[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([[2.8503492 ],\n",
       "        [0.97012275],\n",
       "        [2.0389943 ]], dtype=float32),\n",
       " array([2.063, 1.25 , 1.66 ]))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred, y_test[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This works, but it is sometimes better to build networks with more complex topologies, or with multiple inputs and outputs. In this case, Keras offers the Functional API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "An example of a more complex nonsequential network is a wide & deep neural network. This connects all or part of the inputs directly to the output layer, making it possible for the network to learn deep patterns (using the deep path) and simple rules (using the short path). In contrast, a normal MLP forces all the data to go through every layer of the model, which can distort some of the more simple patterns. \n",
    "                        \n",
    "Deep Path\n",
    "\n",
    "Input Layer --> Hidden 1 --> Hidden 2 --> Concat --> Output Layer\n",
    "\n",
    "Wide Path          \n",
    "\n",
    "Input Layer --------------> Concat --> Output Layer\n",
    "\n",
    "The wide path sidesteps the hidden layers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ = keras.layers.Input(shape=X_train.shape[1:])\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.Concatenate()([input_, hidden2])\n",
    "output = keras.layers.Dense(1)(concat)\n",
    "model = keras.Model(inputs=[input_], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. First an ```input``` object is create. This specifies the shape and datatype of the input the model will get. Sometimes a model can have multiple inputs.\n",
    "2. Next is a dense hidden layer with 30 neurons. Once it is created, we call it like a function and pass it in the input layer. This is why this is called the Functional API. Right now we are just telling keras how the model should be connected, no data is being passed yet. \n",
    "3. Then a second hidden layer is created, which is passed the output first hidden layer.\n",
    "4. The we create a ```concatenate``` layer, and again immediately use it like a function to concatenate the input and the output of the second hidden layer. \n",
    "5. Then we create the output layer with a single neuron and no activation function (regression), which is passed the result of the concatenation.\n",
    "6. Then the model is created, specifying which inputs and outputs to use. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the model is created, the rest of the process is the same."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.8329 - val_loss: 0.5902\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 1.0498 - val_loss: 0.5629\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5365 - val_loss: 0.4517\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5188 - val_loss: 0.5221\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.5011 - val_loss: 0.4328\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4585 - val_loss: 0.4636\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4431 - val_loss: 0.5304\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.6317 - val_loss: 0.4290\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4136 - val_loss: 0.7517\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.4243 - val_loss: 0.6102\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4022 - val_loss: 0.8614\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.7984 - val_loss: 0.7740\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4211 - val_loss: 0.7058\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3874 - val_loss: 0.8947\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3929 - val_loss: 0.8777\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3670 - val_loss: 1.4726\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.5714 - val_loss: 1.3530\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3575 - val_loss: 1.8269\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.7149 - val_loss: 1.6544\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 0s 1ms/step - loss: 0.3991 - val_loss: 1.7189\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss='mean_squared_error', optimizer='sgd'\n",
    ")\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=20, validation_data=(X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 967us/step - loss: 0.3991\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But what if you want to send a subset of features through the wide path, and another subset (possibly overlapping) through the deep path? \n",
    "\n",
    "Input B --> Hidden 1 --> Hidden 2 --> Concat --> Output\n",
    "\n",
    "Input A -----------> Concat --> Output\n",
    "\n",
    "Suppose we want to send 5 features [0,4] through the wide path, and 6 [2,7] through the deep path. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "output = keras.layers.Dense(1, name='output')(concat)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Its good convention to at least name the most important layers once the model begins to get more complex. Now we can compile the model as usual, but when we call the ```fit()``` method, we pass a pair of matricies (X_train_A, X_train_B), one per input. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\ander\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\keras\\optimizer_v2\\gradient_descent.py:102: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  super(SGD, self).__init__(name, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    loss = 'mse', optimizer=keras.optimizers.SGD(lr=1e-3)\n",
    ")\n",
    "\n",
    "X_train_A , X_train_B = X_train[:, :5], X_train[:, 2:]\n",
    "X_valid_A, X_valid_B = X_valid[:, :5], X_valid[:, 2:]\n",
    "X_test_A, X_test_B = X_test[:, :5], X_test[:, 2:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4841 - val_loss: 0.4057\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4821 - val_loss: 0.4035\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4796 - val_loss: 0.4051\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4778 - val_loss: 0.4030\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4752 - val_loss: 0.4051\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4730 - val_loss: 0.4038\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4720 - val_loss: 0.4077\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4694 - val_loss: 0.4127\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4682 - val_loss: 0.4091\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4661 - val_loss: 0.4112\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.4644 - val_loss: 0.4108\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4629 - val_loss: 0.4113\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4617 - val_loss: 0.4142\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4599 - val_loss: 0.4192\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4582 - val_loss: 0.4185\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4572 - val_loss: 0.4203\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4556 - val_loss: 0.4325\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4545 - val_loss: 0.4317\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4531 - val_loss: 0.4362\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4517 - val_loss: 0.4368\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    (X_train_A, X_train_B), y_train, epochs=20, validation_data=((X_valid_A, X_valid_B), y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.4656\n"
     ]
    }
   ],
   "source": [
    "mse_test = model.evaluate((X_test_A, X_test_B), y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use cases for having multiple outputs:\n",
    "\n",
    "- The task may demand it. For instance, you may want to locate and identify the main object in a picture. This is a regression task (finding the coordinates of the object's center, as well as its width and height) and a classification task\n",
    "- You also may have multiple independent tasks based on the same data. You can get better results on all tasks by training a single network with one output per task. This is because the network can learn features in the data that are useful across tasks. For example, you could perform multitask classification on pictures of faces by classifying the person's facial expression, and if they are wearing glasses or not. \n",
    "- It can also be used as a regularization technique (ex. a training constraint whose objective is to reduce overfitting). For example, you may want to add some auxiliary outputs in a neural network to ensure that the underlying part of the network learns something useful on its own without relying on the rest of the network.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adding extra outputs to the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same archetecture as above up to the main output layer\n",
    "input_A = keras.layers.Input(shape=[5], name='wide_input')\n",
    "input_B = keras.layers.Input(shape=[6], name='deep_input')\n",
    "hidden1 = keras.layers.Dense(30, activation='relu')(input_B)\n",
    "hidden2 = keras.layers.Dense(30, activation='relu')(hidden1)\n",
    "concat = keras.layers.concatenate([input_A, hidden2])\n",
    "#\n",
    "output = keras.layers.Dense(1, name='main_output')(concat)\n",
    "aux_output = keras.layers.Dense(1, name='aux_output')(hidden2)\n",
    "model = keras.Model(inputs=[input_A, input_B], outputs=[output, aux_output])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each output needs its own loss function. When we compile the model, we should pass a list of losses (if its a single loss, keras assumes this will be used for all outputs). Keras will, by default, add up all these losses to get the final loss during training. We care much more about the loss in the main output, so we give this value more weight. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    loss=['mse', 'mse'], loss_weights= [0.9, 0.1], optimizer='sgd'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to provide labels for each output during training. Since in this example both outputs are trying to predict the same thing, we should use the same labels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3908 - main_output_loss: 0.3756 - aux_output_loss: 0.5274 - val_loss: 2.1252 - val_main_output_loss: 2.1373 - val_aux_output_loss: 2.0167\n",
      "Epoch 2/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.4161 - main_output_loss: 0.4046 - aux_output_loss: 0.5196 - val_loss: 2.2801 - val_main_output_loss: 2.2878 - val_aux_output_loss: 2.2108\n",
      "Epoch 3/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3979 - main_output_loss: 0.3835 - aux_output_loss: 0.5274 - val_loss: 2.5320 - val_main_output_loss: 2.5380 - val_aux_output_loss: 2.4781\n",
      "Epoch 4/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3756 - main_output_loss: 0.3614 - aux_output_loss: 0.5042 - val_loss: 2.8134 - val_main_output_loss: 2.8315 - val_aux_output_loss: 2.6503\n",
      "Epoch 5/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3723 - main_output_loss: 0.3584 - aux_output_loss: 0.4973 - val_loss: 2.6663 - val_main_output_loss: 2.6642 - val_aux_output_loss: 2.6849\n",
      "Epoch 6/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3736 - main_output_loss: 0.3606 - aux_output_loss: 0.4908 - val_loss: 2.8999 - val_main_output_loss: 2.9004 - val_aux_output_loss: 2.8954\n",
      "Epoch 7/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3669 - main_output_loss: 0.3537 - aux_output_loss: 0.4854 - val_loss: 3.1831 - val_main_output_loss: 3.1870 - val_aux_output_loss: 3.1476\n",
      "Epoch 8/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3671 - main_output_loss: 0.3542 - aux_output_loss: 0.4827 - val_loss: 3.2857 - val_main_output_loss: 3.2879 - val_aux_output_loss: 3.2659\n",
      "Epoch 9/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3625 - main_output_loss: 0.3500 - aux_output_loss: 0.4752 - val_loss: 3.7328 - val_main_output_loss: 3.7380 - val_aux_output_loss: 3.6864\n",
      "Epoch 10/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3619 - main_output_loss: 0.3496 - aux_output_loss: 0.4733 - val_loss: 3.7753 - val_main_output_loss: 3.7830 - val_aux_output_loss: 3.7061\n",
      "Epoch 11/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3596 - main_output_loss: 0.3474 - aux_output_loss: 0.4696 - val_loss: 3.9408 - val_main_output_loss: 3.9423 - val_aux_output_loss: 3.9272\n",
      "Epoch 12/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3581 - main_output_loss: 0.3462 - aux_output_loss: 0.4656 - val_loss: 3.8856 - val_main_output_loss: 3.8796 - val_aux_output_loss: 3.9392\n",
      "Epoch 13/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3591 - main_output_loss: 0.3475 - aux_output_loss: 0.4630 - val_loss: 4.1410 - val_main_output_loss: 4.1301 - val_aux_output_loss: 4.2390\n",
      "Epoch 14/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3553 - main_output_loss: 0.3439 - aux_output_loss: 0.4581 - val_loss: 3.9444 - val_main_output_loss: 3.9209 - val_aux_output_loss: 4.1562\n",
      "Epoch 15/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3529 - main_output_loss: 0.3417 - aux_output_loss: 0.4537 - val_loss: 4.5130 - val_main_output_loss: 4.5015 - val_aux_output_loss: 4.6164\n",
      "Epoch 16/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3543 - main_output_loss: 0.3435 - aux_output_loss: 0.4519 - val_loss: 4.3975 - val_main_output_loss: 4.3752 - val_aux_output_loss: 4.5977\n",
      "Epoch 17/20\n",
      "363/363 [==============================] - 1s 1ms/step - loss: 0.3517 - main_output_loss: 0.3408 - aux_output_loss: 0.4496 - val_loss: 4.8212 - val_main_output_loss: 4.8146 - val_aux_output_loss: 4.8808\n",
      "Epoch 18/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3548 - main_output_loss: 0.3443 - aux_output_loss: 0.4493 - val_loss: 4.5648 - val_main_output_loss: 4.5374 - val_aux_output_loss: 4.8111\n",
      "Epoch 19/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3500 - main_output_loss: 0.3394 - aux_output_loss: 0.4449 - val_loss: 4.9341 - val_main_output_loss: 4.9103 - val_aux_output_loss: 5.1484\n",
      "Epoch 20/20\n",
      "363/363 [==============================] - 1s 2ms/step - loss: 0.3489 - main_output_loss: 0.3386 - aux_output_loss: 0.4419 - val_loss: 4.7450 - val_main_output_loss: 4.7219 - val_aux_output_loss: 4.9528\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    [X_train_A, X_train_B], [y_train, y_train], epochs=20, validation_data=([X_valid_A, X_valid_B], [y_valid, y_valid])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "162/162 [==============================] - 0s 1ms/step - loss: 0.3983 - main_output_loss: 0.3837 - aux_output_loss: 0.5299\n"
     ]
    }
   ],
   "source": [
    "total_loss, main_loss, aux_loss = model.evaluate(\n",
    "    [X_test_A, X_test_B], [y_test, y_test]\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4daf73df99b5d5ee04b9c4f6d0c928016b99f4a7167499c60f06ba788794ec50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
