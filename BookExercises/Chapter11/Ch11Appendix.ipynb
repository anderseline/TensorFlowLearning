{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 11 - Deep Network on CIFAR10 Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import ssl\n",
    "import matplotlib.pyplot as plt \n",
    "from functools import partial \n",
    "import os\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "ssl._create_default_https_context = ssl._create_unverified_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(X_train_full, y_train_full), (X_test, y_test) = keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train, X_valid, y_valid = X_train_full[:40000], y_train_full[:40000], X_train_full[40000:], y_train_full[40000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Default DNN Architecture \n",
    "- LeCun Initialiation\n",
    "- ELU Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "DenseLayer = partial(\n",
    "    keras.layers.Dense, \n",
    "    activation = 'elu',\n",
    "    kernel_initializer = 'he_normal'\n",
    ")\n",
    "\n",
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32, 32, 3]))\n",
    "for _ in range(20):\n",
    "    model.add(DenseLayer(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 3072)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               307300    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 500,210\n",
      "Trainable params: 500,210\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_cb = keras.callbacks.EarlyStopping(patience=20)\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('models/ch11_model.h5', save_best_only=True)\n",
    "run_index = 1\n",
    "run_logdir = os.path.join(os.curdir, 'cifar10_logs', 'run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)\n",
    "callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# default learning rate works alright, but not as good as 5e-5\n",
    "model.compile(loss=\"sparse_categorical_crossentropy\",\n",
    "              optimizer=keras.optimizers.Nadam(learning_rate=5e-5),\n",
    "              metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 18s 12ms/step - loss: 4.7200 - accuracy: 0.1420 - val_loss: 2.2326 - val_accuracy: 0.1847\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 2.1254 - accuracy: 0.2159 - val_loss: 2.0535 - val_accuracy: 0.2408\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.9986 - accuracy: 0.2634 - val_loss: 1.9302 - val_accuracy: 0.2881\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.9035 - accuracy: 0.3004 - val_loss: 1.8619 - val_accuracy: 0.3117\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.8382 - accuracy: 0.3273 - val_loss: 1.8116 - val_accuracy: 0.3422\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.7964 - accuracy: 0.3431 - val_loss: 1.8478 - val_accuracy: 0.3218\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.7539 - accuracy: 0.3616 - val_loss: 1.7696 - val_accuracy: 0.3538\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.7253 - accuracy: 0.3743 - val_loss: 1.7767 - val_accuracy: 0.3600\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6888 - accuracy: 0.3842 - val_loss: 1.7090 - val_accuracy: 0.3810\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.6660 - accuracy: 0.3964 - val_loss: 1.6943 - val_accuracy: 0.3867\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.6428 - accuracy: 0.4044 - val_loss: 1.6746 - val_accuracy: 0.4054\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.6193 - accuracy: 0.4131 - val_loss: 1.6636 - val_accuracy: 0.4011\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.6012 - accuracy: 0.4199 - val_loss: 1.6632 - val_accuracy: 0.3992\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5755 - accuracy: 0.4287 - val_loss: 1.6444 - val_accuracy: 0.4101\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5584 - accuracy: 0.4363 - val_loss: 1.6392 - val_accuracy: 0.4156\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.5426 - accuracy: 0.4429 - val_loss: 1.6461 - val_accuracy: 0.4099\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.5260 - accuracy: 0.4488 - val_loss: 1.6200 - val_accuracy: 0.4213\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.5073 - accuracy: 0.4555 - val_loss: 1.6239 - val_accuracy: 0.4189\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4993 - accuracy: 0.4579 - val_loss: 1.6022 - val_accuracy: 0.4275\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4837 - accuracy: 0.4647 - val_loss: 1.6069 - val_accuracy: 0.4203\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4727 - accuracy: 0.4650 - val_loss: 1.6355 - val_accuracy: 0.4102\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.4559 - accuracy: 0.4752 - val_loss: 1.5798 - val_accuracy: 0.4385\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4442 - accuracy: 0.4770 - val_loss: 1.5938 - val_accuracy: 0.4362\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.4333 - accuracy: 0.4799 - val_loss: 1.5896 - val_accuracy: 0.4382\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.4201 - accuracy: 0.4868 - val_loss: 1.5684 - val_accuracy: 0.4469\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.4092 - accuracy: 0.4922 - val_loss: 1.5687 - val_accuracy: 0.4441\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3975 - accuracy: 0.4943 - val_loss: 1.5771 - val_accuracy: 0.4460\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.3849 - accuracy: 0.5008 - val_loss: 1.5917 - val_accuracy: 0.4387\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3771 - accuracy: 0.5039 - val_loss: 1.5637 - val_accuracy: 0.4509\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3663 - accuracy: 0.5067 - val_loss: 1.5682 - val_accuracy: 0.4465\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3537 - accuracy: 0.5121 - val_loss: 1.5648 - val_accuracy: 0.4507\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3425 - accuracy: 0.5148 - val_loss: 1.5674 - val_accuracy: 0.4514\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3370 - accuracy: 0.5191 - val_loss: 1.5571 - val_accuracy: 0.4546\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3230 - accuracy: 0.5215 - val_loss: 1.5574 - val_accuracy: 0.4513\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.3143 - accuracy: 0.5264 - val_loss: 1.5708 - val_accuracy: 0.4458\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.3035 - accuracy: 0.5329 - val_loss: 1.5787 - val_accuracy: 0.4508\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2947 - accuracy: 0.5352 - val_loss: 1.5614 - val_accuracy: 0.4577\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.2878 - accuracy: 0.5372 - val_loss: 1.5798 - val_accuracy: 0.4501\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2790 - accuracy: 0.5378 - val_loss: 1.5672 - val_accuracy: 0.4585\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2669 - accuracy: 0.5437 - val_loss: 1.5878 - val_accuracy: 0.4526\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2621 - accuracy: 0.5445 - val_loss: 1.5870 - val_accuracy: 0.4582\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2505 - accuracy: 0.5491 - val_loss: 1.6052 - val_accuracy: 0.4483\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2414 - accuracy: 0.5508 - val_loss: 1.5933 - val_accuracy: 0.4551\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2324 - accuracy: 0.5544 - val_loss: 1.6106 - val_accuracy: 0.4506\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.2268 - accuracy: 0.5563 - val_loss: 1.6215 - val_accuracy: 0.4532\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 16s 12ms/step - loss: 1.2178 - accuracy: 0.5608 - val_loss: 1.6059 - val_accuracy: 0.4589\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.2097 - accuracy: 0.5631 - val_loss: 1.6114 - val_accuracy: 0.4544\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.1995 - accuracy: 0.5674 - val_loss: 1.5953 - val_accuracy: 0.4583\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 16s 13ms/step - loss: 1.1967 - accuracy: 0.5688 - val_loss: 1.6165 - val_accuracy: 0.4517\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.1844 - accuracy: 0.5730 - val_loss: 1.6207 - val_accuracy: 0.4561\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 18s 14ms/step - loss: 1.1786 - accuracy: 0.5734 - val_loss: 1.6139 - val_accuracy: 0.4562\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 15s 12ms/step - loss: 1.1641 - accuracy: 0.5814 - val_loss: 1.6392 - val_accuracy: 0.4534\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 17s 13ms/step - loss: 1.1607 - accuracy: 0.5834 - val_loss: 1.6305 - val_accuracy: 0.4547\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2473926f130>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), callbacks=[callbacks]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 3ms/step - loss: 1.5956 - accuracy: 0.4578\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.5956181287765503, 0.4578000009059906]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Batch Normalization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " flatten (Flatten)           (None, 3072)              0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 3072)             12288     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               307300    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 520,498\n",
      "Trainable params: 510,354\n",
      "Non-trainable params: 10,144\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# callback method to decrease lr exponentially after 10 epochs\n",
    "# NOT FOR FINDING OPTIMAL RATE, METHOD OF TRAINING THAT DECREASES LR AS CONVERGENCE BEGINS\n",
    "def exp_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    return lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_callback = tf.keras.callbacks.LearningRateScheduler(exp_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('models/ch11_model_batchnorm.h5', save_best_only=True)\n",
    "run_logdir = os.path.join(os.curdir, 'cifar10_logs_batchnorm', 'run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer = optimizer, metrics=['accuracy'] \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 35s 20ms/step - loss: 1.8483 - accuracy: 0.3365 - val_loss: 1.6725 - val_accuracy: 0.3963\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.6730 - accuracy: 0.4025 - val_loss: 1.6088 - val_accuracy: 0.4309\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.6026 - accuracy: 0.4291 - val_loss: 1.5504 - val_accuracy: 0.4446\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.5550 - accuracy: 0.4439 - val_loss: 1.4790 - val_accuracy: 0.4730\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.5083 - accuracy: 0.4619 - val_loss: 1.4633 - val_accuracy: 0.4716\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 1.4756 - accuracy: 0.4762 - val_loss: 1.4465 - val_accuracy: 0.4893\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.4425 - accuracy: 0.4877 - val_loss: 1.4169 - val_accuracy: 0.4946\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.4117 - accuracy: 0.4963 - val_loss: 1.3953 - val_accuracy: 0.5023\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.3869 - accuracy: 0.5058 - val_loss: 1.3973 - val_accuracy: 0.5065\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.3642 - accuracy: 0.5155 - val_loss: 1.4005 - val_accuracy: 0.5003\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.3376 - accuracy: 0.5264 - val_loss: 1.3763 - val_accuracy: 0.5180\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.3209 - accuracy: 0.5311 - val_loss: 1.3737 - val_accuracy: 0.5172\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.2981 - accuracy: 0.5401 - val_loss: 1.3832 - val_accuracy: 0.5160\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 1.2838 - accuracy: 0.5462 - val_loss: 1.3788 - val_accuracy: 0.5113\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.2642 - accuracy: 0.5524 - val_loss: 1.3883 - val_accuracy: 0.5101\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.2432 - accuracy: 0.5595 - val_loss: 1.3668 - val_accuracy: 0.5186\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.2287 - accuracy: 0.5642 - val_loss: 1.3602 - val_accuracy: 0.5221\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.2184 - accuracy: 0.5686 - val_loss: 1.3659 - val_accuracy: 0.5215\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.2014 - accuracy: 0.5738 - val_loss: 1.3745 - val_accuracy: 0.5225\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 1.1867 - accuracy: 0.5799 - val_loss: 1.3683 - val_accuracy: 0.5290\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.1683 - accuracy: 0.5877 - val_loss: 1.3877 - val_accuracy: 0.5182\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 23s 19ms/step - loss: 1.1539 - accuracy: 0.5944 - val_loss: 1.3585 - val_accuracy: 0.5323\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.1420 - accuracy: 0.6000 - val_loss: 1.3717 - val_accuracy: 0.5316\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.1246 - accuracy: 0.6016 - val_loss: 1.3529 - val_accuracy: 0.5365\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.1227 - accuracy: 0.6017 - val_loss: 1.3689 - val_accuracy: 0.5278\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 1.1087 - accuracy: 0.6086 - val_loss: 1.3783 - val_accuracy: 0.5255\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 22s 17ms/step - loss: 1.0894 - accuracy: 0.6149 - val_loss: 1.3765 - val_accuracy: 0.5249\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 22s 18ms/step - loss: 1.0788 - accuracy: 0.6190 - val_loss: 1.3582 - val_accuracy: 0.5339\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 23s 18ms/step - loss: 1.0717 - accuracy: 0.6206 - val_loss: 1.3936 - val_accuracy: 0.5283\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0584 - accuracy: 0.6270 - val_loss: 1.3705 - val_accuracy: 0.5364\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0519 - accuracy: 0.6274 - val_loss: 1.3829 - val_accuracy: 0.5300\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0355 - accuracy: 0.6328 - val_loss: 1.3775 - val_accuracy: 0.5361\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0232 - accuracy: 0.6426 - val_loss: 1.4304 - val_accuracy: 0.5235\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 26s 20ms/step - loss: 1.0167 - accuracy: 0.6428 - val_loss: 1.3911 - val_accuracy: 0.5330\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 1.0046 - accuracy: 0.6457 - val_loss: 1.4094 - val_accuracy: 0.5354\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 1.0026 - accuracy: 0.6481 - val_loss: 1.3895 - val_accuracy: 0.5340\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9864 - accuracy: 0.6517 - val_loss: 1.4077 - val_accuracy: 0.5276\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9744 - accuracy: 0.6568 - val_loss: 1.3879 - val_accuracy: 0.5327\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 25s 20ms/step - loss: 0.9700 - accuracy: 0.6569 - val_loss: 1.3920 - val_accuracy: 0.5354\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.9615 - accuracy: 0.6597 - val_loss: 1.4312 - val_accuracy: 0.5308\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 27s 22ms/step - loss: 0.9512 - accuracy: 0.6635 - val_loss: 1.4274 - val_accuracy: 0.5298\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 24s 20ms/step - loss: 0.9369 - accuracy: 0.6706 - val_loss: 1.4150 - val_accuracy: 0.5334\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.9329 - accuracy: 0.6715 - val_loss: 1.4065 - val_accuracy: 0.5414\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 24s 19ms/step - loss: 0.9300 - accuracy: 0.6726 - val_loss: 1.4669 - val_accuracy: 0.5263\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, epochs=100, validation_data=(X_valid, y_valid), \n",
    "    callbacks = [early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 4ms/step - loss: 1.4547 - accuracy: 0.5254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.4547209739685059, 0.5253999829292297]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Batch Norm and Convolutions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(32,32,3)))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D(2,2))\n",
    "model.add(keras.layers.Flatten(input_shape=[32,32,3]))\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='he_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('elu'))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 15, 15, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 6, 6, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 256)              1024      \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 100)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_1 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_2 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_4 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_5 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_4 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_6 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_5 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_7 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_6 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_7 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_8 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_7 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_8 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_9 (Batc  (None, 100)              400       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation_8 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_9 (Dense)             (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_10 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_9 (Activation)   (None, 100)               0         \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_11 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_10 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_12 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_11 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_13 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_12 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_14 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_13 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_15 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_14 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_16 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_15 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_16 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_17 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_16 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_17 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_18 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_17 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_19 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_18 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_19 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_20 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_19 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_20 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 251,218\n",
      "Trainable params: 246,706\n",
      "Non-trainable params: 4,512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "# same learning rate with conv layers?\n",
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('models/ch11_model_batchnorm_conv.h5', save_best_only=True)\n",
    "run_logdir = os.path.join(os.curdir, 'cifar10_logs_batchnorm_conv', 'run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 43s 28ms/step - loss: 1.7685 - accuracy: 0.3529 - val_loss: 1.7412 - val_accuracy: 0.3642\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 1.4785 - accuracy: 0.4668 - val_loss: 1.4275 - val_accuracy: 0.4948\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 1.3685 - accuracy: 0.5109 - val_loss: 1.7144 - val_accuracy: 0.4442\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 1.2860 - accuracy: 0.5414 - val_loss: 1.3042 - val_accuracy: 0.5419\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 1.2132 - accuracy: 0.5699 - val_loss: 1.1431 - val_accuracy: 0.5977\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 34s 28ms/step - loss: 1.1508 - accuracy: 0.5929 - val_loss: 1.6163 - val_accuracy: 0.4944\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 1.1079 - accuracy: 0.6090 - val_loss: 1.3534 - val_accuracy: 0.5230\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 1.0742 - accuracy: 0.6217 - val_loss: 1.1419 - val_accuracy: 0.5989\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 1.0319 - accuracy: 0.6418 - val_loss: 1.1489 - val_accuracy: 0.6078\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 36s 28ms/step - loss: 0.9990 - accuracy: 0.6493 - val_loss: 1.0712 - val_accuracy: 0.6268\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.9656 - accuracy: 0.6633 - val_loss: 1.1040 - val_accuracy: 0.6139\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.9315 - accuracy: 0.6748 - val_loss: 1.1222 - val_accuracy: 0.6193\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.9128 - accuracy: 0.6836 - val_loss: 1.0353 - val_accuracy: 0.6498\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.8901 - accuracy: 0.6913 - val_loss: 1.0161 - val_accuracy: 0.6481\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.8667 - accuracy: 0.6999 - val_loss: 1.0836 - val_accuracy: 0.6342\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.8443 - accuracy: 0.7064 - val_loss: 1.0171 - val_accuracy: 0.6552\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.8172 - accuracy: 0.7179 - val_loss: 1.0021 - val_accuracy: 0.6602\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.8023 - accuracy: 0.7241 - val_loss: 1.0656 - val_accuracy: 0.6484\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.7846 - accuracy: 0.7279 - val_loss: 0.9989 - val_accuracy: 0.6734\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7653 - accuracy: 0.7332 - val_loss: 1.1168 - val_accuracy: 0.6216\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.7446 - accuracy: 0.7400 - val_loss: 0.9794 - val_accuracy: 0.6575\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 38s 30ms/step - loss: 0.7250 - accuracy: 0.7495 - val_loss: 1.0293 - val_accuracy: 0.6608\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7091 - accuracy: 0.7536 - val_loss: 1.0795 - val_accuracy: 0.6574\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 37s 29ms/step - loss: 0.6973 - accuracy: 0.7615 - val_loss: 1.1132 - val_accuracy: 0.6350\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 41s 33ms/step - loss: 0.6838 - accuracy: 0.7646 - val_loss: 0.9833 - val_accuracy: 0.6721\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.6717 - accuracy: 0.7685 - val_loss: 1.0748 - val_accuracy: 0.6528\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.6610 - accuracy: 0.7741 - val_loss: 0.9906 - val_accuracy: 0.6727\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.6454 - accuracy: 0.7759 - val_loss: 1.0274 - val_accuracy: 0.6636\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.6328 - accuracy: 0.7802 - val_loss: 0.9700 - val_accuracy: 0.6832\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.6231 - accuracy: 0.7856 - val_loss: 0.9394 - val_accuracy: 0.6953\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.6227 - accuracy: 0.7852 - val_loss: 1.0376 - val_accuracy: 0.6717\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.6028 - accuracy: 0.7923 - val_loss: 0.9532 - val_accuracy: 0.6878\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.5939 - accuracy: 0.7958 - val_loss: 0.9425 - val_accuracy: 0.6880\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.5812 - accuracy: 0.7999 - val_loss: 0.9767 - val_accuracy: 0.6885\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.5721 - accuracy: 0.8053 - val_loss: 0.9412 - val_accuracy: 0.6972\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.5674 - accuracy: 0.8051 - val_loss: 1.0103 - val_accuracy: 0.6815\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.5554 - accuracy: 0.8083 - val_loss: 1.0075 - val_accuracy: 0.6833\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.5489 - accuracy: 0.8111 - val_loss: 0.9977 - val_accuracy: 0.6863\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5356 - accuracy: 0.8161 - val_loss: 1.0885 - val_accuracy: 0.6619\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5321 - accuracy: 0.8165 - val_loss: 0.9599 - val_accuracy: 0.6897\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5221 - accuracy: 0.8218 - val_loss: 1.1535 - val_accuracy: 0.6499\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 33s 26ms/step - loss: 0.5087 - accuracy: 0.8252 - val_loss: 1.0652 - val_accuracy: 0.6734\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.5063 - accuracy: 0.8232 - val_loss: 1.0452 - val_accuracy: 0.6794\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.5067 - accuracy: 0.8255 - val_loss: 1.0732 - val_accuracy: 0.6691\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.4987 - accuracy: 0.8264 - val_loss: 1.1399 - val_accuracy: 0.6536\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 36s 28ms/step - loss: 0.4908 - accuracy: 0.8322 - val_loss: 1.0509 - val_accuracy: 0.6810\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 34s 28ms/step - loss: 0.4825 - accuracy: 0.8349 - val_loss: 1.0548 - val_accuracy: 0.6733\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.4790 - accuracy: 0.8357 - val_loss: 1.0099 - val_accuracy: 0.6860\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.4761 - accuracy: 0.8349 - val_loss: 1.0336 - val_accuracy: 0.6779\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.4665 - accuracy: 0.8396 - val_loss: 1.0454 - val_accuracy: 0.6851\n"
     ]
    }
   ],
   "source": [
    "conv_hisotry = model.fit(\n",
    "    X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 8ms/step - loss: 1.0585 - accuracy: 0.6794\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0584701299667358, 0.6794000267982483]"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add dropout to model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(32,32,3)))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='lecun_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('selu'))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d_6 (Conv2D)           (None, 30, 30, 16)        448       \n",
      "                                                                 \n",
      " max_pooling2d_6 (MaxPooling  (None, 15, 15, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 13, 13, 32)        4640      \n",
      "                                                                 \n",
      " max_pooling2d_7 (MaxPooling  (None, 6, 6, 32)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_8 (Conv2D)           (None, 4, 4, 64)          18496     \n",
      "                                                                 \n",
      " max_pooling2d_8 (MaxPooling  (None, 2, 2, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten_1 (Flatten)         (None, 256)               0         \n",
      "                                                                 \n",
      " batch_normalization_21 (Bat  (None, 256)              1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dense_21 (Dense)            (None, 100)               25700     \n",
      "                                                                 \n",
      " batch_normalization_22 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_20 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_22 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_23 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_21 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_23 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_24 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_22 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_24 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_25 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_23 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_25 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_26 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_24 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_26 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_27 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_25 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_27 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_28 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_26 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_28 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_29 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_27 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_29 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_30 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_28 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_30 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_31 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_29 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_31 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_32 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_30 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_32 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_33 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_31 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_33 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_34 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_32 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_34 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_35 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_33 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_35 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_36 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_34 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_36 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_37 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_35 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_37 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_38 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_36 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_38 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_39 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_37 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_39 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_40 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_38 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " dense_40 (Dense)            (None, 100)               10100     \n",
      "                                                                 \n",
      " batch_normalization_41 (Bat  (None, 100)              400       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " activation_39 (Activation)  (None, 100)               0         \n",
      "                                                                 \n",
      " alpha_dropout_1 (AlphaDropo  (None, 100)              0         \n",
      " ut)                                                             \n",
      "                                                                 \n",
      " dense_41 (Dense)            (None, 10)                1010      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 251,218\n",
      "Trainable params: 246,706\n",
      "Non-trainable params: 4,512\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Nadam(learning_rate=5e-4)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_index = 1\n",
    "model_checkpoint_cb = keras.callbacks.ModelCheckpoint('models/ch11_model_batchnorm_conv_dropout.h5', save_best_only=True)\n",
    "run_logdir = os.path.join(os.curdir, 'cifar10_logs_batchnorm_conv_dropout', 'run_{:03d}'.format(run_index))\n",
    "tensorboard_cb = keras.callbacks.TensorBoard(run_logdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "1250/1250 [==============================] - 42s 27ms/step - loss: 1.8669 - accuracy: 0.3314 - val_loss: 1.5860 - val_accuracy: 0.4210\n",
      "Epoch 2/100\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 1.5683 - accuracy: 0.4399 - val_loss: 1.5094 - val_accuracy: 0.4649\n",
      "Epoch 3/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 1.4502 - accuracy: 0.4866 - val_loss: 1.5229 - val_accuracy: 0.4927\n",
      "Epoch 4/100\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 1.3633 - accuracy: 0.5201 - val_loss: 2.0071 - val_accuracy: 0.3987\n",
      "Epoch 5/100\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 1.2896 - accuracy: 0.5467 - val_loss: 1.4177 - val_accuracy: 0.5114\n",
      "Epoch 6/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 1.2211 - accuracy: 0.5716 - val_loss: 1.3271 - val_accuracy: 0.5478\n",
      "Epoch 7/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 1.1788 - accuracy: 0.5887 - val_loss: 1.2667 - val_accuracy: 0.5699\n",
      "Epoch 8/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 1.1288 - accuracy: 0.6086 - val_loss: 1.3035 - val_accuracy: 0.5649\n",
      "Epoch 9/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 1.0939 - accuracy: 0.6205 - val_loss: 2.0237 - val_accuracy: 0.3934\n",
      "Epoch 10/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 1.0774 - accuracy: 0.6257 - val_loss: 1.1594 - val_accuracy: 0.6070\n",
      "Epoch 11/100\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 1.0401 - accuracy: 0.6403 - val_loss: 1.5718 - val_accuracy: 0.5225\n",
      "Epoch 12/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 1.0055 - accuracy: 0.6505 - val_loss: 1.3823 - val_accuracy: 0.5330\n",
      "Epoch 13/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 1.1548 - accuracy: 0.6019 - val_loss: 1.2511 - val_accuracy: 0.5889\n",
      "Epoch 14/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 1.0075 - accuracy: 0.6528 - val_loss: 1.1863 - val_accuracy: 0.6163\n",
      "Epoch 15/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9613 - accuracy: 0.6668 - val_loss: 1.2247 - val_accuracy: 0.5993\n",
      "Epoch 16/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9452 - accuracy: 0.6730 - val_loss: 1.1334 - val_accuracy: 0.6329\n",
      "Epoch 17/100\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 0.9178 - accuracy: 0.6834 - val_loss: 1.1091 - val_accuracy: 0.6306\n",
      "Epoch 18/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.9042 - accuracy: 0.6881 - val_loss: 1.1013 - val_accuracy: 0.6366\n",
      "Epoch 19/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8906 - accuracy: 0.6952 - val_loss: 1.0097 - val_accuracy: 0.6691\n",
      "Epoch 20/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8849 - accuracy: 0.6962 - val_loss: 1.0389 - val_accuracy: 0.6593\n",
      "Epoch 21/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8513 - accuracy: 0.7056 - val_loss: 1.1776 - val_accuracy: 0.6185\n",
      "Epoch 22/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.8328 - accuracy: 0.7145 - val_loss: 1.1623 - val_accuracy: 0.6509\n",
      "Epoch 23/100\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.8228 - accuracy: 0.7151 - val_loss: 1.4008 - val_accuracy: 0.5798\n",
      "Epoch 24/100\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.8110 - accuracy: 0.7199 - val_loss: 1.0071 - val_accuracy: 0.6750\n",
      "Epoch 25/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.7956 - accuracy: 0.7276 - val_loss: 1.0395 - val_accuracy: 0.6659\n",
      "Epoch 26/100\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.7904 - accuracy: 0.7289 - val_loss: 1.0090 - val_accuracy: 0.6777\n",
      "Epoch 27/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.7789 - accuracy: 0.7323 - val_loss: 1.1306 - val_accuracy: 0.6519\n",
      "Epoch 28/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.7539 - accuracy: 0.7411 - val_loss: 1.1822 - val_accuracy: 0.6451\n",
      "Epoch 29/100\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.7406 - accuracy: 0.7465 - val_loss: 1.0838 - val_accuracy: 0.6543\n",
      "Epoch 30/100\n",
      "1250/1250 [==============================] - 33s 27ms/step - loss: 0.7362 - accuracy: 0.7496 - val_loss: 1.0379 - val_accuracy: 0.6864\n",
      "Epoch 31/100\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.7266 - accuracy: 0.7518 - val_loss: 1.2801 - val_accuracy: 0.6285\n",
      "Epoch 32/100\n",
      "1250/1250 [==============================] - 34s 28ms/step - loss: 0.7097 - accuracy: 0.7556 - val_loss: 1.1082 - val_accuracy: 0.6616\n",
      "Epoch 33/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.7002 - accuracy: 0.7606 - val_loss: 1.1783 - val_accuracy: 0.6582\n",
      "Epoch 34/100\n",
      "1250/1250 [==============================] - 34s 27ms/step - loss: 0.6920 - accuracy: 0.7624 - val_loss: 1.0470 - val_accuracy: 0.6775\n",
      "Epoch 35/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6796 - accuracy: 0.7678 - val_loss: 1.0009 - val_accuracy: 0.6888\n",
      "Epoch 36/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6825 - accuracy: 0.7676 - val_loss: 1.1197 - val_accuracy: 0.6647\n",
      "Epoch 37/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6640 - accuracy: 0.7725 - val_loss: 1.0995 - val_accuracy: 0.6886\n",
      "Epoch 38/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6602 - accuracy: 0.7725 - val_loss: 1.1624 - val_accuracy: 0.6604\n",
      "Epoch 39/100\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6472 - accuracy: 0.7789 - val_loss: 0.9988 - val_accuracy: 0.6958\n",
      "Epoch 40/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6409 - accuracy: 0.7815 - val_loss: 1.1534 - val_accuracy: 0.6717\n",
      "Epoch 41/100\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.6380 - accuracy: 0.7836 - val_loss: 1.1343 - val_accuracy: 0.6753\n",
      "Epoch 42/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6464 - accuracy: 0.7814 - val_loss: 0.9756 - val_accuracy: 0.7025\n",
      "Epoch 43/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6196 - accuracy: 0.7895 - val_loss: 1.0931 - val_accuracy: 0.6834\n",
      "Epoch 44/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6180 - accuracy: 0.7867 - val_loss: 1.4658 - val_accuracy: 0.6259\n",
      "Epoch 45/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.6239 - accuracy: 0.7860 - val_loss: 1.1684 - val_accuracy: 0.6471\n",
      "Epoch 46/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.5973 - accuracy: 0.7944 - val_loss: 1.0193 - val_accuracy: 0.7019\n",
      "Epoch 47/100\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.5890 - accuracy: 0.8004 - val_loss: 1.0547 - val_accuracy: 0.6951\n",
      "Epoch 48/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.5889 - accuracy: 0.7988 - val_loss: 1.0296 - val_accuracy: 0.6976\n",
      "Epoch 49/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.5880 - accuracy: 0.7998 - val_loss: 1.1081 - val_accuracy: 0.6800\n",
      "Epoch 50/100\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 0.5735 - accuracy: 0.8067 - val_loss: 1.0282 - val_accuracy: 0.7014\n",
      "Epoch 51/100\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.5646 - accuracy: 0.8083 - val_loss: 1.2475 - val_accuracy: 0.6634\n",
      "Epoch 52/100\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.5592 - accuracy: 0.8098 - val_loss: 1.0422 - val_accuracy: 0.6997\n",
      "Epoch 53/100\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.6235 - accuracy: 0.7874 - val_loss: 1.0643 - val_accuracy: 0.6879\n",
      "Epoch 54/100\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.5480 - accuracy: 0.8123 - val_loss: 1.1031 - val_accuracy: 0.6783\n",
      "Epoch 55/100\n",
      "1250/1250 [==============================] - 30s 24ms/step - loss: 0.5406 - accuracy: 0.8166 - val_loss: 1.1537 - val_accuracy: 0.6855\n",
      "Epoch 56/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.5429 - accuracy: 0.8139 - val_loss: 1.1016 - val_accuracy: 0.6929\n",
      "Epoch 57/100\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.5379 - accuracy: 0.8160 - val_loss: 1.1532 - val_accuracy: 0.6873\n",
      "Epoch 58/100\n",
      "1250/1250 [==============================] - 31s 25ms/step - loss: 0.5240 - accuracy: 0.8195 - val_loss: 1.3873 - val_accuracy: 0.6384\n",
      "Epoch 59/100\n",
      "1250/1250 [==============================] - 32s 26ms/step - loss: 0.5161 - accuracy: 0.8226 - val_loss: 1.0393 - val_accuracy: 0.7061\n",
      "Epoch 60/100\n",
      "1250/1250 [==============================] - 32s 25ms/step - loss: 0.5148 - accuracy: 0.8237 - val_loss: 1.2394 - val_accuracy: 0.6583\n",
      "Epoch 61/100\n",
      "1250/1250 [==============================] - 35s 28ms/step - loss: 0.5108 - accuracy: 0.8261 - val_loss: 1.1998 - val_accuracy: 0.6687\n",
      "Epoch 62/100\n",
      "1250/1250 [==============================] - 36s 29ms/step - loss: 0.5077 - accuracy: 0.8262 - val_loss: 1.0721 - val_accuracy: 0.6996\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(\n",
    "    X_train, y_train, epochs=100, validation_data=(X_valid, y_valid),\n",
    "    callbacks=[early_stopping_cb, model_checkpoint_cb, tensorboard_cb]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 3s 8ms/step - loss: 1.0986 - accuracy: 0.7021\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0986346006393433, 0.7020999789237976]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MC Dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MCAlphaDropout(keras.layers.AlphaDropout):\n",
    "    def call(self, inputs):\n",
    "        return super().call(inputs, training=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mc_model = keras.models.Sequential([\n",
    "    MCAlphaDropout(layer.rate) if isinstance(layer, keras.layers.AlphaDropout) else layer\n",
    "    for layer in model.layers\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mc_dropout_predict_probs(mc_model, X, n_samples=10):\n",
    "    Y_probs = [mc_model.predict(X) for sample in range(n_samples)]\n",
    "    return np.mean(Y_probs, axis=0)\n",
    "\n",
    "def mc_dropout_predict_classes(mc_model, X, n_samples=10):\n",
    "    Y_probs = mc_dropout_predict_probs(mc_model, X, n_samples)\n",
    "    return np.argmax(Y_probs, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7014"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "y_pred = mc_dropout_predict_classes(mc_model, X_valid)\n",
    "accuracy = np.mean(y_pred == y_valid[:, 0])\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1cycle Scheduling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "K = keras.backend\n",
    "\n",
    "class ExponentialLearningRate(keras.callbacks.Callback):\n",
    "    def __init__(self, factor):\n",
    "        self.factor = factor\n",
    "        self.rates = []\n",
    "        self.losses = []\n",
    "    def on_batch_end(self, batch, logs):\n",
    "        self.rates.append(K.get_value(self.model.optimizer.learning_rate))\n",
    "        self.losses.append(logs['loss'])\n",
    "        K.set_value(self.model.optimizer.learning_rate, self.model.optimizer.learning_rate * self.factor)\n",
    "\n",
    "def find_learning_rate(model, X, y, epochs=1, batch_size=32, min_rate=10**-5, max_rate=10):\n",
    "    init_weights = model.get_weights()\n",
    "    iterations = math.ceil(len(X) / batch_size) * epochs\n",
    "    factor = np.exp(np.log(max_rate / min_rate) / iterations)\n",
    "    init_lr = K.get_value(model.optimizer.learning_rate)\n",
    "    K.set_value(model.optimizer.learning_rate, min_rate)\n",
    "    exp_lr = ExponentialLearningRate(factor)\n",
    "    history = model.fit(X, y, epochs=epochs, batch_size=batch_size, callbacks=[exp_lr])\n",
    "    K.set_value(model.optimizer.learning_rate, init_lr)\n",
    "    model.set_weights(init_weights)\n",
    "    return exp_lr.rates, exp_lr.losses\n",
    "\n",
    "def plot_lr_vs_loss(rates, losses):\n",
    "    plt.plot(rates, losses)\n",
    "    plt.gca().set_xscale('log')\n",
    "    plt.hlines(min(losses), min(rates), max(rates))\n",
    "    plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 2])\n",
    "    plt.xlabel(\"Learning rate\")\n",
    "    plt.ylabel(\"Loss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OneCycleScheduler(keras.callbacks.Callback):\n",
    "    def __init__(self, iterations, max_rate, start_rate=None,\n",
    "                 last_iterations=None, last_rate=None):\n",
    "        self.iterations = iterations\n",
    "        self.max_rate = max_rate\n",
    "        self.start_rate = start_rate or max_rate / 10\n",
    "        self.last_iterations = last_iterations or iterations // 10 + 1\n",
    "        self.half_iteration = (iterations - self.last_iterations) // 2\n",
    "        self.last_rate = last_rate or self.start_rate / 1000\n",
    "        self.iteration = 0\n",
    "    def _interpolate(self, iter1, iter2, rate1, rate2):\n",
    "        return ((rate2 - rate1) * (self.iteration - iter1)\n",
    "                / (iter2 - iter1) + rate1)\n",
    "    def on_batch_begin(self, batch, logs):\n",
    "        if self.iteration < self.half_iteration:\n",
    "            rate = self._interpolate(0, self.half_iteration, self.start_rate, self.max_rate)\n",
    "        elif self.iteration < 2 * self.half_iteration:\n",
    "            rate = self._interpolate(self.half_iteration, 2 * self.half_iteration,\n",
    "                                     self.max_rate, self.start_rate)\n",
    "        else:\n",
    "            rate = self._interpolate(2 * self.half_iteration, self.iterations,\n",
    "                                     self.start_rate, self.last_rate)\n",
    "        self.iteration += 1\n",
    "        K.set_value(self.model.optimizer.learning_rate, rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.backend.clear_session()\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.models.Sequential()\n",
    "model.add(keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(32,32,3)))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(32, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Conv2D(64, (3,3), activation='relu'))\n",
    "model.add(keras.layers.MaxPool2D((2,2)))\n",
    "model.add(keras.layers.Flatten())\n",
    "model.add(keras.layers.BatchNormalization())\n",
    "for _ in range(20):\n",
    "    model.add(keras.layers.Dense(100, kernel_initializer='lecun_normal'))\n",
    "    model.add(keras.layers.BatchNormalization())\n",
    "    model.add(keras.layers.Activation('selu'))\n",
    "model.add(keras.layers.AlphaDropout(rate=0.1))\n",
    "model.add(keras.layers.Dense(10, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 20s 53ms/step - loss: 14.3551 - accuracy: 0.1330\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(9.999999747378752e-06,\n",
       " 9.568211555480957,\n",
       " 2.7706503868103027,\n",
       " 4.27955116544451)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_size=128\n",
    "rates, losses = find_learning_rate(model, X_train, y_train, epochs=1, batch_size=batch_size)\n",
    "plot_lr_vs_loss(rates, losses)\n",
    "plt.axis([min(rates), max(rates), min(losses), (losses[0] + min(losses)) / 1.4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = 1e-2\n",
    "optimizer = keras.optimizers.SGD(learning_rate=1e-2)\n",
    "model.compile(\n",
    "    loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15\n",
      "313/313 [==============================] - 23s 63ms/step - loss: 2.1540 - accuracy: 0.2566 - val_loss: 2.1726 - val_accuracy: 0.2327\n",
      "Epoch 2/15\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 1.7346 - accuracy: 0.3821 - val_loss: 2.0433 - val_accuracy: 0.3414\n",
      "Epoch 3/15\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 1.5544 - accuracy: 0.4472 - val_loss: 2.0232 - val_accuracy: 0.3317\n",
      "Epoch 4/15\n",
      "313/313 [==============================] - 21s 66ms/step - loss: 1.4420 - accuracy: 0.4906 - val_loss: 1.7491 - val_accuracy: 0.4096\n",
      "Epoch 5/15\n",
      "313/313 [==============================] - 21s 69ms/step - loss: 1.3507 - accuracy: 0.5229 - val_loss: 2.6748 - val_accuracy: 0.2927\n",
      "Epoch 6/15\n",
      "313/313 [==============================] - 21s 69ms/step - loss: 1.2806 - accuracy: 0.5483 - val_loss: 1.8825 - val_accuracy: 0.3849\n",
      "Epoch 7/15\n",
      "313/313 [==============================] - 21s 67ms/step - loss: 1.2293 - accuracy: 0.5705 - val_loss: 1.7696 - val_accuracy: 0.4125\n",
      "Epoch 8/15\n",
      "313/313 [==============================] - 23s 74ms/step - loss: 1.1543 - accuracy: 0.5974 - val_loss: 1.9994 - val_accuracy: 0.4472\n",
      "Epoch 9/15\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 1.0908 - accuracy: 0.6181 - val_loss: 1.7712 - val_accuracy: 0.4422\n",
      "Epoch 10/15\n",
      "313/313 [==============================] - 21s 68ms/step - loss: 1.0356 - accuracy: 0.6392 - val_loss: 1.3222 - val_accuracy: 0.5710\n",
      "Epoch 11/15\n",
      "313/313 [==============================] - 22s 69ms/step - loss: 0.9837 - accuracy: 0.6573 - val_loss: 1.3689 - val_accuracy: 0.5502\n",
      "Epoch 12/15\n",
      "313/313 [==============================] - 22s 70ms/step - loss: 0.9365 - accuracy: 0.6722 - val_loss: 1.1418 - val_accuracy: 0.6177\n",
      "Epoch 13/15\n",
      "313/313 [==============================] - 23s 75ms/step - loss: 0.8953 - accuracy: 0.6885 - val_loss: 1.1639 - val_accuracy: 0.6234\n",
      "Epoch 14/15\n",
      "313/313 [==============================] - 21s 67ms/step - loss: 0.8545 - accuracy: 0.7003 - val_loss: 1.0554 - val_accuracy: 0.6512\n",
      "Epoch 15/15\n",
      "313/313 [==============================] - 21s 67ms/step - loss: 0.8284 - accuracy: 0.7124 - val_loss: 1.0438 - val_accuracy: 0.6551\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 15\n",
    "onecycle = OneCycleScheduler(math.ceil(len(X_train) / batch_size) * n_epochs, max_rate=0.05)\n",
    "history = model.fit(\n",
    "    X_train, y_train, epochs=n_epochs, batch_size=batch_size, validation_data=(X_valid, y_valid), callbacks=[onecycle]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 7ms/step - loss: 1.0539 - accuracy: 0.6500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0538513660430908, 0.6499999761581421]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(X_test, y_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.1 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "4daf73df99b5d5ee04b9c4f6d0c928016b99f4a7167499c60f06ba788794ec50"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
